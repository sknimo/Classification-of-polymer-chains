{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "693f40c7",
   "metadata": {},
   "source": [
    "# Peak deconvolution using clustering from Machine learning\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Peak deconvolution is sometimes required as part of our work as research scientists and we've relied on commercial softwares for such tasks. Recently I found myself working with a challenging material, one that was difficult to deconvolute. With the production of materials exhibiting complex behaviours by day, tasks such as deconvolutions is also getting a bit tricky. In this article, I illustrate how machine learning can be employed to help in such a task.\n",
    "\n",
    "Peak deconvolutions are performed for several reason such as helping us understand the influence of a subgroup on a feature of interest. In polymer science for example while the general response of a material to an imposed stress depends on the ensemble of chains that makes its population, its mechanical properties can be tweaked by the relative populations of high and low molecular weight chains. In my opinion, the challenge that arises with these recent materials from the stand point of deconvolution is as a result of the user's inability to accurately provide the initial guesses on the sub-population's characteristics. By making use of clustering which is an unsupervised machine learning technique to identify those sub-populations  difficult to spot with the eye, this problem can be circumvented.\n",
    "\n",
    "The data used for this illustration was digitized from an article which can be found <u> here </u>. It represents the distribution of polymer chains referred to as molecular weight distributions (MWD). \n",
    "\n",
    "These article is structured as follows:\n",
    "1. The challenges faced during deconvolution would first be discussed.\n",
    "2. Attention would next be given to the preprocessing steps. A step  necessary since the process of obtaining the data through digitization resulted in fewer datapoints which were not evenly distributed within the sample space.\n",
    "3. Clustering would subsequently be performed on the preprocessed dataset. \n",
    "4. And an optimization routine would finally be used minimize the error between the sum of the deconvoluted peaks and the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8226adef",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "In the figure shown below, the curve with legend _MWD_ is the original distribution and _peak1_ and _peak2_ are the deconvoluted outputs. A few things are noted from the deconvoluted output:\n",
    "1. Truncation of the original distribution at the tails.\n",
    "2. A baseline higher than that of the original distribution.\n",
    "3. Peak locations that do not really coincide with that of  the original distribution.\n",
    "\n",
    "The aim of this article is to highlight ways to improve the deconvolution by making use of clustering to obtain the initial guesses and then proceed with optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9178ba",
   "metadata": {},
   "source": [
    "Let us start by importing the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2655fd9a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmixture\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GaussianMixture \n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m curve_fit\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.mixture import GaussianMixture \n",
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6d0574",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "#loading our file\n",
    "fname ='distribution.xlsx'\n",
    "digitized_data = pd.read_excel(fname)\n",
    "digitized_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffdc281",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(digitized_data.iloc[:,0],digitized_data.iloc[:,1],'k',label='MWD')\n",
    "ax.plot(digitized_data.iloc[:,0],digitized_data.iloc[:,2],'b',alpha=0.6,label='peak 1')\n",
    "ax.plot(digitized_data.iloc[:,0],digitized_data.iloc[:,3],'m',alpha=0.6,label='peak 2')\n",
    "ax.legend()\n",
    "ax.set_xlabel('$Log \\; M$')\n",
    "ax.set_ylabel('$dw/dLogM$')\n",
    "ax.set_title('Figure 1')\n",
    "# plt.savefig('original_distro.png',dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15eb7629",
   "metadata": {},
   "source": [
    "## Preprocessing step\n",
    "\n",
    "To deal with the problem of having fewer datapoints which are unevenly distributed within the sample space, we would need to perform learning on the original distribution and then generate a new set of reliable datapoints. The Gaussian Mixture Model (GMM) would be used for this purpose. Even though GMM is mostly used for clustering, it is actually a density estimator under the hood. So in our example it is well suited to perform both distribution learning and clustering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb3126f",
   "metadata": {},
   "outputs": [],
   "source": [
    "digitized_data = digitized_data.drop(['peak 1', 'peak 2'],axis=1)\n",
    "digitized_data = digitized_data.sort_values(by=['logMW'], ascending=False)\n",
    "digitized_data.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01344013",
   "metadata": {},
   "source": [
    "We will use the Gaussian mixture model from the sklearn and specify the covariance type as full, which allows us the flexibility to fit elliptical shapes if necessary instead of constraining it to circular shapes. We subsequently fit it to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6353280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating a the model and fitting it to the data\n",
    "gmm_learning =GaussianMixture(n_components=len(digitized_data), covariance_type='full').fit(digitized_data.dropna().values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c57b59",
   "metadata": {},
   "source": [
    "After learning from the distribution, we then generate 1000 data points which would be used for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3506c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating 1000 data points based on the learning\n",
    "generated_distribution = gmm_learning.sample(1000)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ef89f0",
   "metadata": {},
   "source": [
    "The first component of the output contains the generated samples and the second the labels. Visualizing the output of the randomly generated samples shows a perfect agreement with the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb5c3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(digitized_data.iloc[:,0],digitized_data.iloc[:,1],'k',label='original')\n",
    "plt.plot(generated_distribution[:,0],generated_distribution[:,1],'r.',label='generated')\n",
    "plt.legend()\n",
    "plt.title('Figure 2')\n",
    "# plt.savefig('generated_points.png',dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ba6db0",
   "metadata": {},
   "source": [
    "## Getting the initial guesses\n",
    "\n",
    "With the generated dataset we evaluate the initial guesses by performing clustering. The difference between the use of GMM in this step and the previous is that, in the previous step we fitted one gaussian to every data point in effect it meant that every datapoint belonged to its own cluster with an infinite likehood. In this step however, by providing a fewer number of clusters each data point would be assigned to one of the clusters based on probability. For example for 3 clusters and 100 datapoints, each datapoint would be assigned to one of the 3 clusters based on some calculated probability.\n",
    "\n",
    "Let us write a function for that. It would have as input parameters, the generated data and the number of clusters to fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6afb2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmmClustering(generateddata,nComponents):\n",
    "    \"\"\"Performs clustering on given distribution\n",
    "    Args:\n",
    "        generatedData(df): columns =[LogM,dwdlogM]\n",
    "        nComponents(int): the number of clusters to identify\n",
    "    returns: \n",
    "        list: [weight, mean,standard deviation] of each cluster \n",
    "    \"\"\"\n",
    "    # creating a model and fitting the model to the data\n",
    "    gmm =GaussianMixture(n_components=nComponents, covariance_type='full').fit(generateddata) \n",
    "    mean = gmm.means_[:,0] # the first columns contains the means   \n",
    "    #covariance returns a set of arrays each for one cluster. The main diagonal contains the variance\n",
    "    covariance = gmm.covariances_    \n",
    "    # the weights here are the population fractions\n",
    "    weight = gmm.weights_\n",
    "    \n",
    "    cluster_parms =[] # to store the cluster parameters\n",
    "    for i in range(nComponents):\n",
    "        cluster_parms.append([weight[i],mean[i],np.sqrt(np.diag(covariance[i]))[0]])\n",
    "    return list(np.hstack(cluster_parms))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e3c09f",
   "metadata": {},
   "source": [
    "The function above (gmmClustering) would return the estimated mean location of each cluster, the weight fractions and standard deviations. To observe the output we now write a function which would accept the evaluated clusters parameters and generate the gaussian curve.\n",
    "\n",
    "So we next write a function for the gaussian curves. This function would accept any number of parameters and will return the sum of the gaussians as a distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c91f5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing the gaussian function\n",
    "def gaussian_func(x,*params):\n",
    "    \"\"\" generate the gaussian distribution\"\"\"\n",
    "    y = np.zeros_like(x)\n",
    "    for i in range(0,len(params),3):\n",
    "        amp= params[i] # amplitude\n",
    "        ctr = params[i+1] # peak location\n",
    "        wid = params[i+2] # standard deviation\n",
    "        y += amp*np.exp(-((x-ctr)/wid)**2)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e49bd1",
   "metadata": {},
   "source": [
    "Armed with these 2 functions let us now test it on 2 clusters. That is to say, we assume that the distribution is made up of two subpopulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafd1341",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_clusters = 2\n",
    "clusters_params2 = gmmClustering(generated_distribution,nComponents=number_of_clusters)\n",
    "print(clusters_params2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2252b762",
   "metadata": {},
   "source": [
    "The outputs are the cluster's weight fractions, peak location and standard deviation respectively.\n",
    "With these initial guesses we can now reconstruct the distribution using the gaussian_func function and minimize the error between the reconstructed distribution generated and the original data.\n",
    "\n",
    "We would need an optimization routine for that and would use the curve_fit function from sklearn.optimize. Let's write the optimizeGmmParameter function to handle that. It would return the adjusted cluster parameters that would result in the minimal error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eccf15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizeGmmParameter(rawdata, gmm_parameters,gaussFunct):\n",
    "    \"\"\"Optimize parameters obtained from the gmm model\n",
    "    Args:\n",
    "        rawData(df): raw data\n",
    "        gmm_parameters[list]: list of init parameters obtained from gmm\n",
    "        gaussFunct(functions): gaussian functions to be used to perform curve optimization\n",
    "    returns:\n",
    "        array: [amp, peak location, width]\n",
    "    \"\"\"\n",
    "    x_raw = rawdata.iloc[:,0].dropna().values;y_raw=rawdata.iloc[:,1].dropna().values\n",
    "    \n",
    "    # returns the fitting parameters, accepts the function,data and init_para\n",
    "    popt_gauss, pcov_gauss = curve_fit(gaussFunct, x_raw, y_raw, p0=gmm_parameters, maxfev = 50000)\n",
    "    optimizedParameter = np.asarray(popt_gauss).reshape(-1,3)\n",
    "    return sorted(optimizedParameter, key= lambda x: x[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31e7bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_parms2 = optimizeGmmParameter(digitized_data, clusters_params2,gaussian_func)\n",
    "print(optimized_parms2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dbf84b",
   "metadata": {},
   "source": [
    "Now observing the deconvoluted peaks (figure 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b501233b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(digitized_data.iloc[:,0],digitized_data.iloc[:,1],'k', lw=3,alpha=0.3)\n",
    "for i in range(len(optimized_parms2)):\n",
    "    plt.plot(digitized_data.iloc[:,0],gaussian_func(digitized_data.iloc[:,0],*optimized_parms2[i]),lw=3)\n",
    "\n",
    "plt.xlabel('$Log \\; M$')\n",
    "plt.ylabel('$dw/dLogM$')\n",
    "plt.title('Figure 3')\n",
    "# plt.savefig('two_clusters.png',dpi=300,bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79167c0b",
   "metadata": {},
   "source": [
    "The output shown in figure 3 is not perfect possibly indicating that there could be more than 2 clusters. Increasing the number of clusters to 3 produces the results in figure 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f35a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing clustering\n",
    "number_of_clusters = 3\n",
    "clusters_params3 = gmmClustering(generated_distribution,nComponents=number_of_clusters)\n",
    "# performing curve fitting\n",
    "optimized_parms3 = optimizeGmmParameter(digitized_data, clusters_params3,gaussian_func)\n",
    "# observing the output\n",
    "plt.plot(digitized_data.iloc[:,0],digitized_data.iloc[:,1],'k', lw=3,alpha=0.3)\n",
    "for i in range(len(optimized_parms3)):\n",
    "    plt.plot(digitized_data.iloc[:,0],gaussian_func(digitized_data.iloc[:,0],*optimized_parms3[i]),lw=3)\n",
    "\n",
    "plt.xlabel('$Log \\; M$')\n",
    "plt.ylabel('$dw/dLogM$')\n",
    "plt.title('Figure 4')\n",
    "# plt.savefig('three_clusters.png',dpi=300,bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b056d3",
   "metadata": {},
   "source": [
    "At this point we can combine the first 2 peaks to yield figure 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10104a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = digitized_data.iloc[:,0]\n",
    "lower_portion = gaussian_func(x_values,*optimized_parms3[0])+ gaussian_func(digitized_data.iloc[:,0],*optimized_parms3[1])\n",
    "higher_portion = gaussian_func(x_values,*optimized_parms3[-1])\n",
    "\n",
    "plt.plot(x_values,lower_portion)\n",
    "plt.plot(x_values,higher_portion)\n",
    "plt.plot(x_values,digitized_data.iloc[:,1],'k', lw=3,alpha=0.3)\n",
    "plt.title('Figure 5')\n",
    "# plt.savefig('finalOutput.png',dpi=300,bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85db77fe",
   "metadata": {},
   "source": [
    "As shown, the deconvolution looks much better. \n",
    "Clustering was only used to help us get the initial guess, so we won't be making use of metrics such as BIC or AIC to evaluate the best number of clusters that fits well, so we would instead make use of a for loop and iterate over a possible number of clusters and optimize the output. All put together as a function is presented below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568c4814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def peak_deconvolution(originalData,generatedData,n_clusters,gaussian_func):\n",
    "    n_components =np.arange(2,n_clusters)\n",
    "    par_list = []\n",
    "    for n in n_clusters:\n",
    "\n",
    "        init_guess = gmmClustering(generatedData,n)\n",
    "        opt_param = optimizeGmmParameter(originalData, init_guess,gaussian_func)\n",
    "        par_list.append(opt_param)\n",
    "        plt.plot(df.iloc[:,0],df.iloc[:,1],'k', alpha=0.3,label='orig')\n",
    "\n",
    "        for i in range(len(opt_param)):\n",
    "            a = gausfunc(df.iloc[:,0],*opt_param[i])\n",
    "            plt.plot(df.iloc[:,0],a)\n",
    "        plt.xlabel('$Log \\; M$')\n",
    "        plt.ylabel('$dw/dLogM$')\n",
    "        plt.show()\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce35cb1a",
   "metadata": {},
   "source": [
    "In this article, it was shown that peak deconvolution can be greatly improve with the use of clustering. Thanks for your attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da33295",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
